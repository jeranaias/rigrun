# rigrun - Self-Hosted LLM Router | OpenAI-Compatible Local AI

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Release](https://img.shields.io/github/v/release/rigrun/rigrun?color=orange)](https://github.com/rigrun/rigrun/releases)
[![Rust](https://img.shields.io/badge/rust-2021-orange.svg)](https://www.rust-lang.org)
[![Stars](https://img.shields.io/github/stars/rigrun/rigrun?style=social)](https://github.com/rigrun/rigrun)

```
   ____  _       ____
  |  _ \(_) __ _|  _ \ _   _ _ __
  | |_) | |/ _` | |_) | | | | '_ \
  |  _ <| | (_| |  _ <| |_| | | | |
  |_| \_\_|\__, |_| \_\\__,_|_| |_|
           |___/  v0.1.0
```

**Reduce LLM costs by 90% with local GPU inference.** OpenAI-compatible API router for self-hosted AI that intelligently routes requests through semantic cache ‚Üí local LLM (Ollama) ‚Üí cloud fallback. Drop-in GPT alternative and Claude API alternative for developers who want privacy, performance, and cost savings.

---

## What is rigrun?

**For Developers:**
rigrun is an OpenAI-compatible API router that runs on your hardware. It reduces LLM costs by intelligently routing requests through a three-tier system: semantic cache (instant, free) ‚Üí local GPU inference (fast, free) ‚Üí cloud fallback (only when needed). Think of it as a smart proxy that saves you money while maintaining compatibility with existing OpenAI/Claude codebases.

**For Non-Technical Users:**
Instead of paying cloud services every time you use AI (like ChatGPT), rigrun runs AI models on your own computer. It's like having your own private ChatGPT that:
- Costs 90% less than cloud services
- Keeps your data private on your machine
- Works offline (with optional cloud backup)
- Runs faster for common questions

**How It Works:**
1. **Cache First**: Answers similar questions instantly (40-60% of requests)
2. **Your GPU Second**: Runs AI on your computer's graphics card (30-50% of requests)
3. **Cloud Last**: Only pays for complex questions that need extra power (10% of requests)

**Result**: You get the same AI capabilities for 1/10th the cost, with better privacy.

---

## 5-Minute Quickstart

### Step 1: Install Ollama
Ollama runs AI models on your computer.

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from https://ollama.com/download

### Step 2: Install rigrun
Choose the easiest option for you:

**Option A: Pre-built Binary (Recommended)**
1. Download from https://github.com/rigrun/rigrun/releases
2. Extract and move to your PATH
3. Done!

**Option B: Install via Cargo (if you have Rust)**
```bash
cargo install rigrun
```

### Step 3: Run rigrun
```bash
rigrun
```

That's it! rigrun will:
- Detect your GPU automatically
- Download the best AI model for your hardware (one-time, takes 5-10 minutes)
- Start an API server at http://localhost:8787

### Step 4: Make Your First Request

**Using cURL:**
```bash
curl http://localhost:8787/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

**Using Python:**
```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8787/v1", api_key="unused")
response = client.chat.completions.create(
    model="auto",
    messages=[{"role": "user", "content": "Say hello!"}]
)
print(response.choices[0].message.content)
```

Congratulations! You just ran your first local AI query.

**Next Steps:**
- [Getting Started Guide](docs/getting-started.md) - Complete walkthrough
- [Installation Guide](docs/installation.md) - Detailed installation for all platforms
- [Configuration](docs/configuration.md) - Customize rigrun for your needs

---

## üåê OpenAI-Compatible API Endpoints

```bash
# Health check
curl http://localhost:8787/health

# List models
curl http://localhost:8787/v1/models

# Chat completions
curl http://localhost:8787/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# Stats & cache
curl http://localhost:8787/stats
curl http://localhost:8787/cache/stats
```

### üêç Python Integration
```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8787/v1", api_key="unused")
response = client.chat.completions.create(
    model="auto",
    messages=[{"role": "user", "content": "Write Python code for fizzbuzz"}]
)
print(response.choices[0].message.content)
```

### üìú JavaScript Integration
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'http://localhost:8787/v1',
  apiKey: 'unused',
});

const response = await openai.chat.completions.create({
  model: 'auto',
  messages: [{ role: 'user', content: 'Explain async/await' }],
});
console.log(response.choices[0].message.content);
```

---

## üéÆ GPU-Optimized Model Recommendations

| VRAM | Recommended Model | Notes |
|------|-------------------|-------|
| <6GB | `qwen2.5-coder:3b` | Lightweight, fast |
| 6-8GB | `qwen2.5-coder:7b` | Good balance |
| 9-16GB | `qwen2.5-coder:14b` | Recommended |
| 17GB+ | `deepseek-coder-v2:16b` or `llama3.3:70b` | Professional / Maximum capability |

```bash
# Pull specific model
rigrun pull qwen2.5-coder:7b

# List available models
rigrun models
```

---

## üí∞ Real Benchmarks: How Much You Save with Local LLM

### Side-by-Side Cost Comparison (1M tokens/month)

| Provider | Architecture | Monthly Cost | Savings vs GPT-4 |
|----------|--------------|--------------|------------------|
| OpenAI GPT-4 | 100% cloud | **$30.00** | Baseline |
| Claude 3.5 Sonnet | 100% cloud | **$15.00** | 50% |
| OpenRouter Mixtral | 100% cloud | **$12.00** | 60% |
| **rigrun (90% local)** | Cache + GPU + Cloud | **$1.20** | **96% savings** |

### Real Developer Example (10M tokens/month)
**Before rigrun** (OpenAI GPT-4):
- Monthly cost: **$300**
- Annual cost: **$3,600**

**After rigrun** (90% local GPU, 10% cloud):
- Monthly cost: **$30** (90% handled by your GPU)
- Annual cost: **$360**
- **Annual savings: $3,240** üí∞

**ROI**: A $1,500 GPU pays for itself in 5 months vs OpenAI API costs

### Where the Savings Come From
1. **Semantic Cache** (40-60% hit rate) ‚Üí $0 cost
2. **Local GPU Inference** (30-50% of requests) ‚Üí $0 cost after hardware
3. **Cloud Fallback** (only 10% of requests) ‚Üí Pay only for what you need

---

## CLI Commands

```bash
rigrun              # Start server
rigrun --paranoid   # Start server in paranoid mode (no cloud)
rigrun status       # Show live stats and GPU info
rigrun config       # Configure settings
rigrun models       # List available models
rigrun pull <model> # Download specific model
rigrun chat         # Interactive chat session
rigrun ide-setup    # Configure VS Code/Cursor/JetBrains
rigrun gpu-setup    # GPU setup wizard
rigrun export       # Export your data (cache, audit log, stats)
```

---

## üî• Key Features - Why Developers Choose rigrun

### 1. Intelligent LLM Request Routing
Three-tier architecture for maximum cost efficiency:
1. **Semantic Cache Layer** - Instant responses for similar queries ($0 cost)
2. **Local GPU Layer** - Self-hosted inference via Ollama API ($0 marginal cost)
3. **Cloud Fallback Layer** - OpenRouter for complex queries (pay per use only)

**Example**: 100 API calls ‚Üí 60 from cache + 30 from local GPU + 10 from cloud = **90% cost reduction**

### 2. Smart Semantic Caching (Not Just Key-Value)
Context-aware deduplication using embeddings:
- Recognizes similar queries: "What is recursion?" ‚âà "Explain recursion to me"
- Configurable TTL (default: 24 hours)
- Automatic persistence across restarts
- Works with any LLM model (GPT, Claude, local models)

**Cache hit rate**: 40-60% typical for development workflows

### 3. Zero-Config GPU Auto-Detection
One command to rule them all:
- **Detects GPU**: NVIDIA (CUDA), AMD (ROCm), Apple Silicon (Metal), Intel Arc
- **Recommends optimal Ollama model** based on your VRAM
- **Auto-downloads model** from Ollama registry
- **VRAM monitoring**: Warns before out-of-memory errors

**Supported models**: Qwen2.5-Coder, DeepSeek-Coder-V2, Llama 3.3, and 100+ more

### 4. Real-Time Cost Tracking & Analytics
Monitor every dollar saved:
- **Live dashboard**: Cache hits, local inference, cloud calls
- **Cost calculator**: Compare vs OpenAI/Claude/Anthropic pricing
- **Daily/weekly reports**: Track savings over time
- **Prometheus-compatible metrics** via `/stats` endpoint

**Typical savings report**: "Saved $245 this month by handling 87% of requests locally"

---

## ‚öôÔ∏è Configuration

### Quick Config
```bash
# Set OpenRouter key for cloud fallback
rigrun config --openrouter-key sk-or-xxx

# Change default model
rigrun config --model qwen2.5-coder:14b

# Change port
rigrun config --port 8080

# View current config
rigrun config --show
```

### Config File
Edit `~/.rigrun/config.json`:
```json
{
  "openrouter_key": "sk-or-xxx",
  "model": "qwen2.5-coder:7b",
  "port": 8787
}
```

---

## üìä Monitoring

```bash
rigrun status
```

Example output:
```
=== RigRun Status ===

‚úì Server: Running on port 8787
i Model: qwen2.5-coder:14b
i GPU: NVIDIA RTX 4090 (24GB)
i VRAM: 4096MB / 24576MB (16.7% used)

=== GPU Utilization ===

  qwen2.5-coder:14b (8.2 GB) - GPU: 100%

=== Today's Stats ===

  Total queries:  147
  Local queries:  132
  Cloud queries:  15
  Money saved:    $23.45
```

---

## üîå IDE Integration

rigrun works seamlessly with popular IDEs:

```bash
rigrun ide-setup
```

Supports:
- **VS Code** - Configures Copilot/Continue extension
- **Cursor** - Sets up custom model endpoint
- **JetBrains** (IntelliJ, PyCharm, WebStorm, etc.) - AI Assistant configuration
- **Neovim** - Copilot.lua / codecompanion.nvim setup

The setup wizard auto-generates configurations using your local AI!

---

## Who Uses rigrun?

rigrun is useful for:

- **Indie Developers**: Building AI features without high API costs
- **Startups**: Reducing AI infrastructure expenses
- **Enterprise Teams**: Self-hosted AI for compliance requirements
- **Open Source Projects**: Running LLM-powered tools affordably
- **Data Scientists**: Local experimentation without cloud bills

### Example Savings

The following are hypothetical examples of potential savings (actual results vary based on usage patterns):

- A developer handling 10M tokens/month could reduce costs from ~$300/month (cloud-only) to ~$30/month (90% local)
- Teams with repetitive queries see higher cache hit rates and greater savings

**Have real numbers to share?** [Open a discussion](https://github.com/rigrun/rigrun/discussions) with your experience.

---

## üèÜ Why rigrun vs Alternatives?

| Feature | rigrun | LiteLLM | OpenAI Proxy | Raw Ollama |
|---------|--------|---------|--------------|------------|
| **Semantic Caching** | ‚úÖ Built-in | ‚ùå | ‚ùå | ‚ùå |
| **GPU Auto-detection** | ‚úÖ | ‚ùå | ‚ùå | ‚ö†Ô∏è Manual |
| **Cost Tracking** | ‚úÖ Real-time | ‚ö†Ô∏è Basic | ‚ùå | ‚ùå |
| **Cloud Fallback** | ‚úÖ Smart routing | ‚ö†Ô∏è Manual | ‚úÖ | ‚ùå |
| **Zero Config** | ‚úÖ 3 commands | ‚ùå Complex | ‚ùå | ‚ö†Ô∏è Moderate |
| **OpenAI Compatible** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |

**TLDR**: rigrun = Ollama + Smart Caching + Cloud Fallback + Cost Tracking in one tool

---

## ü§ù Contributing

We welcome contributions! Here's how to get started:

1. **Fork & clone**
   ```bash
   git clone https://github.com/rigrun/rigrun
   cd rigrun
   ```

2. **Create feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```

3. **Make changes and test**
   ```bash
   cargo test
   cargo build --release
   ```

4. **Commit & push**
   ```bash
   git commit -m "Add amazing feature"
   git push origin feature/amazing-feature
   ```

5. **Open Pull Request**

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

**First-time contributor?** Look for issues tagged with `good-first-issue`!

---

## Privacy & Data Ownership

rigrun is designed with **privacy maximalism** in mind. Your data is yours.

### What Data rigrun Stores Locally

| Location | Data | Purpose |
|----------|------|---------|
| `~/.rigrun/config.json` | API keys, model preferences | Configuration |
| `~/.rigrun/stats.json` | Query counts, cost savings | Analytics |
| `~/.rigrun/audit.log` | Query log with timestamps | Transparency |
| `~/.rigrun/cache/` | Cached responses | Performance |

### What Data Could Go to Cloud

| Scenario | Data Sent | How to Prevent |
|----------|-----------|----------------|
| Cloud fallback (complex queries) | Query text, model response | Use `--paranoid` flag |
| OpenRouter API calls | Full conversation | Don't configure OpenRouter key |
| Explicit cloud model requests | Query text | Use `model: local` or `auto` |

### Paranoid Mode: 100% Local Operation

```bash
# Block ALL cloud requests - your data NEVER leaves your machine
rigrun --paranoid

# Or set in config for permanent paranoid mode
# Add to ~/.rigrun/config.json:
# "paranoid_mode": true
```

When paranoid mode is enabled:
- All cloud requests are **blocked** and return an error
- Only local inference (Ollama) and cache are used
- A warning banner is displayed on startup
- Blocked requests are logged in the audit log

### Audit Logging

Every query is logged to `~/.rigrun/audit.log` for full transparency:

```
2024-01-15 10:23:45 |     CACHE_HIT | "What is recursi..." | 0 tokens | $0.00
2024-01-15 10:24:12 |         LOCAL | "Explain async/a..." | 847 tokens | $0.00
2024-01-15 10:25:33 | CLOUD_BLOCKED | "Design a microservices..." | 0 tokens | $0.00
```

To disable audit logging, add to config: `"audit_log_enabled": false`

### Export & Delete Your Data

```bash
# Export all your data (cache, audit log, stats)
rigrun export

# Export to specific directory
rigrun export --output ~/my-backup/

# Delete all rigrun data
rm -rf ~/.rigrun
rm -rf ~/AppData/Local/rigrun  # Windows
```

### Privacy Configuration Summary

```json
// ~/.rigrun/config.json
{
  "audit_log_enabled": true,   // Log all queries (default: true)
  "paranoid_mode": false       // Block cloud requests (default: false)
}
```

---

## Requirements

- **Rust** - https://rustup.rs (required for `cargo install`)
- **Ollama** - https://ollama.com/download (required for local inference)
- **GPU** (optional but recommended) - NVIDIA, AMD, Apple Silicon, or Intel Arc
- **OpenRouter API Key** (optional) - For cloud fallback only

---

## License

This project is [MIT](LICENSE) licensed - use it anywhere, commercially or personally!

---

## üôè Acknowledgments

- [Ollama](https://ollama.com) - Powering local inference
- [OpenRouter](https://openrouter.ai) - Smart cloud fallback routing
- All our contributors ‚ù§Ô∏è - [See all](https://github.com/rigrun/rigrun/graphs/contributors)

---

## üìö Documentation

Complete documentation is available in the `docs/` folder:

- **[Getting Started](docs/getting-started.md)** - First-run guide with step-by-step setup
- **[Installation](docs/installation.md)** - Detailed installation for all platforms and GPUs
- **[Configuration](docs/configuration.md)** - All configuration options explained
- **[API Reference](docs/api-reference.md)** - Complete API documentation with examples
- **[Troubleshooting](docs/troubleshooting.md)** - Solutions to common problems
- **[Security & Privacy](docs/security.md)** - Authentication, privacy features, and best practices
- **[Contributing](docs/contributing.md)** - Developer setup and contribution guidelines
- **[Changelog](CHANGELOG.md)** - Version history and release notes

### Quick Links

- **Issues**: [GitHub Issues](https://github.com/rigrun/rigrun/issues)
- **Discussions**: [GitHub Discussions](https://github.com/rigrun/rigrun/discussions)
- **Releases**: [GitHub Releases](https://github.com/rigrun/rigrun/releases)

---

## üöÄ Get Started Now

1. **‚≠ê Star this repo** to help others discover cost-effective local LLM solutions
2. **üì• [Download rigrun](https://github.com/rigrun/rigrun/releases)** and install in 3 minutes
3. **üí¨ [Join discussions](https://github.com/rigrun/rigrun/discussions)** to share your cost savings
4. **üêõ [Report issues](https://github.com/rigrun/rigrun/issues)** to help improve rigrun

